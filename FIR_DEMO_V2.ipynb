{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"ytB37MWwq5K4"},"source":["# Demo Facial Image Retrieval"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5Wbyemljr_qB"},"source":["## Loading elements"]},{"cell_type":"code","execution_count":255,"metadata":{"executionInfo":{"elapsed":7676,"status":"ok","timestamp":1675610835100,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"},"user_tz":-60},"id":"yu3Mcu_Tq23l"},"outputs":[],"source":["# Utility\n","import matplotlib.pyplot as plt\n","import os\n","import pandas as pd\n","import numpy as np\n","import subprocess\n","import tkinter as tk\n","import random\n","import zipfile\n","\n","# Image processing\n","import cv2\n","from PIL import Image, ImageTk\n","\n","# KDTree\n","from sklearn.neighbors import KDTree\n","import joblib\n","\n","# Keras\n","# from tensorflow import keras\n","from keras.preprocessing import image as kimage\n","from keras.applications import MobileNetV2, mobilenet_v2"]},{"cell_type":"code","execution_count":256,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20180,"status":"ok","timestamp":1675610855233,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"},"user_tz":-60},"id":"B6FZ9uhfybKF","outputId":"ae7c0915-3aa0-4bc4-aa6e-465117c94e97"},"outputs":[],"source":["path_model = \"Models/Face Image Retrieval/mob_tree.joblib\"\n","path_celeba = \"Datasets/CelebA/celeba.zip\"\n","path_actors = \"Datasets/Demo/Face Image Retrieval/\"\n","path_celeba_attr = \"Datasets/CelebA/list_attr_celeba.csv\"\n","path_actors_attr = \"Datasets/Demo/Face Image Retrieval/list_attr_actors.csv\"\n","path_backgrouds = \"Datasets/Demo/Face Image Retrieval/Backgrounds/\"\n","path_haarcascade = \"Other/haarcascade_frontalface_default.xml\""]},{"cell_type":"markdown","metadata":{"id":"7_vUMlK13-JK"},"source":["Loading the tree."]},{"cell_type":"code","execution_count":257,"metadata":{"executionInfo":{"elapsed":30176,"status":"ok","timestamp":1675610885400,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"},"user_tz":-60},"id":"yvy_1JdTg8Bx"},"outputs":[],"source":["tree = joblib.load(path_model)"]},{"cell_type":"markdown","metadata":{"id":"hOfUPR_i68yS"},"source":["Loading the MobileNetV2."]},{"cell_type":"code","execution_count":258,"metadata":{"id":"bJ3r4UFVrsnN"},"outputs":[],"source":["mobilenet = MobileNetV2(input_shape = (224, 224, 3), weights = 'imagenet', include_top = False, pooling = 'max')"]},{"cell_type":"markdown","metadata":{"id":"n96rMlZN7JQd"},"source":["Loading the dataframes for computing accuracy."]},{"cell_type":"code","execution_count":259,"metadata":{"executionInfo":{"elapsed":1501,"status":"ok","timestamp":1675610891648,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"},"user_tz":-60},"id":"P8IP2P6HKxxu"},"outputs":[],"source":["df_actors = pd.read_csv(path_actors_attr)\n","df_celeba = pd.read_csv(path_celeba_attr)"]},{"cell_type":"code","execution_count":260,"metadata":{},"outputs":[],"source":["actors = [file for file in os.listdir(path_actors) if file.endswith(\".png\")] # actors files list\n","backgrounds = [file for file in os.listdir(path_backgrouds) if file.endswith(\".png\")] # actors files list"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"L3EwsRlA2vaa"},"source":["## Acquisition"]},{"cell_type":"code","execution_count":261,"metadata":{"id":"DZ_caZGo2uY2"},"outputs":[],"source":["root= tk.Tk()\n","\n","canvas1 = tk.Canvas(root, width=400, height=300, relief='raised')\n","canvas1.pack()\n","\n","label1 = tk.Label(root, text='Select face to retrieve')\n","label1.config(font=('helvetica', 16))\n","canvas1.create_window(200, 25, window=label1)\n","\n","label2 = tk.Label(root, text='Number from 0 to 23:')\n","label2.config(font=('helvetica', 11))\n","canvas1.create_window(200, 100, window=label2)\n","\n","def display_text():\n","   global example\n","   example = int(example.get())\n","   root.destroy\n","\n","example = tk.Entry(root)\n","example.pack()\n","canvas1.create_window(200, 140, window=example)\n","\n","    \n","button1 = tk.Button(text='Select', command=lambda: [display_text(), root.destroy()], font=('helvetica', 12, 'bold'))\n","canvas1.create_window(200, 180, window=button1)\n","\n","root.mainloop()"]},{"cell_type":"code","execution_count":262,"metadata":{},"outputs":[],"source":["path_actor_example = path_actors + actors[example]\n","im = Image.open(path_actor_example)"]},{"cell_type":"code","execution_count":263,"metadata":{},"outputs":[],"source":["# Print selected image\n","root = tk.Tk()\n","\n","canvas1 = tk.Canvas(root, width=400, height=250, relief='raised')\n","canvas1.pack()\n","\n","# Root window title and dimension\n","root.title(\"Selected face\")\n","test = ImageTk.PhotoImage(im)\n","label1 = tk.Label(image=test)\n","label1.image = test\n","label1.place(x=0, y=0)\n","\n","button1 = tk.Button(text='Close', command=lambda: root.destroy(), font=('helvetica', 12, 'bold'))\n","\n","root.mainloop()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"c4Gqu4tw2uoE"},"source":["## Processing"]},{"cell_type":"markdown","metadata":{"id":"-WP0oaTebgpW"},"source":["In order to crop the background as much as possible, we use a Haar cascade classifier to detect and crop faces."]},{"cell_type":"code","execution_count":264,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1675613075619,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"},"user_tz":-60},"id":"QJ6G5veEb-1s"},"outputs":[],"source":["# Load the cascade classifier\n","face_cascade = cv2.CascadeClassifier(path_haarcascade)\n","\n","def crop_face(path, scaleFactor, minNeighbors):\n","  # Read the input image\n","  im = cv2.imread(path)\n","  # Convert the image to grayscale\n","  gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n","  # Detect faces in the image\n","  faces = face_cascade.detectMultiScale(gray, scaleFactor, minNeighbors)\n","  if len(faces) == 0:\n","    # We may try to decrease the scaleFactor parameter\n","    faces = face_cascade.detectMultiScale(gray, scaleFactor - 0.1, minNeighbors)\n","    if len(faces) == 0:\n","      return (0, 0)\n","    elif len(faces) > 1:\n","      return (1, 0)\n","    else:\n","      # Draw rectangles around the faces\n","      for (x, y, w, h) in faces:\n","        face = im[y:y+h, x:x+w]\n","        face = cv2.resize(face, (224, 224))\n","        return (2, face)\n","  elif len(faces) > 1:\n","    # We may try to increase the scaleFactor parameter\n","    faces = face_cascade.detectMultiScale(gray, scaleFactor + 0.1, minNeighbors)\n","    if len(faces) == 0:\n","      return (0, 0)\n","    elif len(faces) > 1:\n","      return (1, 0)\n","    else:\n","      # Draw rectangles around the faces\n","      for (x, y, w, h) in faces:\n","        face = im[y:y+h, x:x+w]\n","        face = cv2.resize(face, (224, 224))\n","        return (2, face)\n","  else:\n","  # Draw rectangles around the faces\n","    for (x, y, w, h) in faces:\n","      face = im[y:y+h, x:x+w]\n","      face = cv2.resize(face, (224, 224))\n","      return (2, face)"]},{"cell_type":"markdown","metadata":{"id":"EC644Ixypkz0"},"source":["We crop (by means of the function crop_face) the actor face."]},{"cell_type":"code","execution_count":265,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1675613076970,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"},"user_tz":-60},"id":"PYSJSDVfnk74"},"outputs":[],"source":["res = crop_face(path_actor_example, 1.12, 9)\n","\n","if res[0] == 2:\n","  im = res[1]\n","else:\n","  raise Exception('Didn\\'t found a unique face in the frame')"]},{"cell_type":"markdown","metadata":{"id":"VZ4zsv0Ub_2B"},"source":["Now, we can add a custom background to our query image (i.e. the actor), in order to make it more realistic and less biased."]},{"cell_type":"code","execution_count":266,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1675613078204,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"},"user_tz":-60},"id":"pEf3IRQgc1C9"},"outputs":[],"source":["# First of all we make the background transparent\n","im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n","im = Image.fromarray(im)\n","im = im.convert(\"RGBA\")\n","data = im.getdata()\n","newData = []\n","# Set alpha = 0 if the pixel is white (or almost white)\n","for item in data:\n","  if item[0] >= 240 and item[1] >= 240 and item[2] >= 240:\n","    newData.append((255, 255, 255, 0))\n","  else:\n","    newData.append(item)\n","im.putdata(newData)\n","# Now we paste the actor into the background\n","bg = Image.open(path_backgrouds + backgrounds[example]) \n","# Resize the background image according to the same actor image dimension\n","bg = bg.resize(im.size)\n","bg.paste(im, (0, 0), im)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"U6St5USu0RUn"},"source":["## Query"]},{"cell_type":"markdown","metadata":{"id":"_CjGCY3bvJsJ"},"source":["We query the tree that we previously loaded. Specifically, we take the first 3 nearest-neighbors for each actor."]},{"cell_type":"code","execution_count":267,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1675613079406,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"},"user_tz":-60},"id":"XzbBDcLC0ZNo"},"outputs":[],"source":["def features_func(im):\n","  # Convert into numpy array\n","  x = np.array(im)\n","  # Preprocessing according to MobileNetV2\n","  x = mobilenet_v2.preprocess_input(x)\n","  # Expand dimensions\n","  x = np.expand_dims(x, axis = 0)\n","  # Extract features\n","  feat = mobilenet.predict(x, verbose = False)\n","  # Return features\n","  return feat.flatten()"]},{"cell_type":"code","execution_count":268,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1675613080752,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"},"user_tz":-60},"id":"KZoIZ-9DuoCK"},"outputs":[],"source":["features_actors = features_func(bg)\n","features_actors = np.array(features_actors)"]},{"cell_type":"code","execution_count":269,"metadata":{"executionInfo":{"elapsed":617,"status":"ok","timestamp":1675613081657,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"},"user_tz":-60},"id":"PTThyePOCyqK"},"outputs":[],"source":["dist, ind = tree.query(features_actors.reshape(1, -1), k = 3)\n","ind = ind[0]\n","dist = dist[0]\n","\n","# filenames celebrities retrieved\n","celeb_filenames = ['celeba/' + str(i).zfill(6) + '.jpg' for i in ind]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xahQGsxOHxNX"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":270,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy for image 0: 72.97%\n","Distance for image 0: 77.15\n","\n","Accuracy for image 1: 70.27%\n","Distance for image 1: 77.35\n","\n","Accuracy for image 2: 83.78%\n","Distance for image 2: 77.53\n","\n"]}],"source":["# extraction actor and maches attributes\n","actor_attr = np.array(df_actors.iloc[example, 1:])\n","matches_attr = np.array(df_celeba.iloc[ind, 1:].values)\n","\n","# count of common attributes\n","count = [np.count_nonzero(np.add(actor_attr, match)) for match in matches_attr]\n","\n","# distance and accuracy\n","dist = [round(d, 2) for d in dist]\n","acc = [round((c/len(actor_attr))*100, 2) for c in count]\n","\n","# print accuracies and distances\n","for n, (a, d) in enumerate(zip(acc, dist)):\n","    print(f'Accuracy for image {n}: {a}%')\n","    print(f'Distance for image {n}: {d}\\n')"]},{"cell_type":"code","execution_count":271,"metadata":{},"outputs":[],"source":["root = tk.Tk()\n","root.geometry(\"900x700\")\n","\n","imgs = []\n","with zipfile.ZipFile(path_celeba, mode=\"r\") as archive:\n","    for filename, d, a in zip(celeb_filenames, dist, acc):\n","        # extract images from zip\n","        ext = archive.open(filename)\n","        # load image\n","        imgs.append(ImageTk.PhotoImage(Image.open(ext), master=root))\n","        # composition\n","        label_text= tk.Label(root, text=f'    Accuracy: {a}%\\nDistance: {d}')\n","        label_text.config(font=('helvetica', 14))\n","        label_text.grid()\n","        label_text[\"compound\"] = tk.LEFT\n","        label_text[\"image\"] = imgs[-1]\n","\n","# print actor image\n","a = Image.open(path_actor_example).crop([87, 0, 311, 224])\n","a = a.resize([350,350])\n","act = ImageTk.PhotoImage(a, master=root)\n","lab = tk.Label(root, image=act, width=350, height=350)\n","lab.place(x=500, y=180)    \n","\n","root.mainloop()"]},{"cell_type":"code","execution_count":272,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":608},"executionInfo":{"elapsed":448,"status":"ok","timestamp":1675613085797,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"},"user_tz":-60},"id":"WkkijTCYtuBv","outputId":"d096697c-17e5-478a-a434-fe2cf5d66743"},"outputs":[],"source":["# im_celeb_1 = kimage.load_img('/content/celeb/celeba/' + str(ind[0][0]).zfill(6) + '.jpg', target_size = (224, 224))\n","# im_celeb_2 = kimage.load_img('/content/celeb/celeba/' + str(ind[0][1]).zfill(6) + '.jpg', target_size = (224, 224))\n","# im_celeb_3 = kimage.load_img('/content/celeb/celeba/' + str(ind[0][2]).zfill(6) + '.jpg', target_size = (224, 224))\n","\n","# fig = plt.figure(figsize = (12, 10))\n","\n","# fig.add_subplot(4, 1, 1)\n","# plt.imshow(im)\n","# plt.axis('off')\n","# plt.title('Actor ' + str(example))\n","# fig.add_subplot(4, 1, 2)\n","# plt.imshow(im_celeb_1)\n","# plt.axis('off')\n","# plt.title(str(acc_1) + '%')\n","# fig.add_subplot(4, 1, 3)\n","# plt.imshow(im_celeb_2)\n","# plt.axis('off')\n","# plt.title(str(acc_2) + '%')\n","# fig.add_subplot(4, 1, 4)\n","# plt.imshow(im_celeb_3)\n","# plt.axis('off')\n","# plt.title(str(acc_3) + '%')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}}},"nbformat":4,"nbformat_minor":0}
